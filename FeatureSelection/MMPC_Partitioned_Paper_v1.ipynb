{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.stats import chi2\n",
    "from math import floor\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.svm import LinearSVC\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support as full_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import operator\n",
    "from functools import reduce\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExistDseparator(TargetNode,Xi, Z, X, alpha):\n",
    "    flagExist = False\n",
    "    dsepSet=[]\n",
    "    #counter=0\n",
    "    #print_names(Z)\n",
    "    for i in range(0,(2**len(Z))-1):\n",
    "        IDsubsetZ_dec = i\n",
    "        IDsubsetZ_bin = bin(IDsubsetZ_dec)\n",
    "        subsetZ = getZsubset(IDsubsetZ_bin,Z)\n",
    "        # no cache\n",
    "        #print(\"from exist dseparator\")\n",
    "        dep = Dep(TargetNode,Xi,subsetZ, X, alpha)\n",
    "        #print(subsetZ)\n",
    "        #print(dep)\n",
    "        if (dep==0):\n",
    "            flagExist = True\n",
    "            dsepSet = subsetZ\n",
    "            break\n",
    "    #print(\"Module exist d-separator: \",counter)\n",
    "    return [flagExist,dsepSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCondValues(z, condVars):\n",
    "    if len(condVars)>1:\n",
    "        maxz = reduce(operator.mul,[int(x['name'].split('@')[1]) for x in condVars],1)\n",
    "        d = np.zeros(len(condVars))\n",
    "        \n",
    "        div_dim = maxz/int(condVars[0]['name'].split('@')[1])\n",
    "        num2div = z\n",
    "        for i in range(0,len(condVars)-1):\n",
    "            d[i] = floor(num2div/div_dim)\n",
    "            num2div=num2div%div_dim\n",
    "            div_dim = div_dim/int(condVars[i+1]['name'].split('@')[1])\n",
    "        d[i+1] = num2div\n",
    "    else:\n",
    "        d = [z]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dep(TargetNode, Xi, CondVars, X, alpha):\n",
    "    #print('in Dep')\n",
    "    global data_c_size\n",
    "    \n",
    "    vvTarget = []\n",
    "    for i in range(int(TargetNode['name'].split('@')[1])):\n",
    "        vvTarget.append(i)\n",
    "    \n",
    "    vvXi = []\n",
    "    for i in range(int(Xi['name'].split('@')[1])):\n",
    "        vvXi.append(i)\n",
    "    \n",
    "    szCondVars = 1\n",
    "    #print(CondVars)\n",
    "    for column in CondVars:\n",
    "        #print(column)\n",
    "        szCondVars *= int(column['name'].split('@')[1])\n",
    "    \n",
    "    if data_c_size <= (5 * len(vvTarget)*len(vvXi)*(szCondVars)):\n",
    "        return 1\n",
    "    \n",
    "    if (len(CondVars)==0):\n",
    "        S = np.zeros((len(vvTarget),len(vvXi)))\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                var1=np.array(TargetNode['data']) # Target Node\n",
    "                var2=np.array(Xi['data'])\n",
    "                # looking up for 0, 1 and 2\n",
    "                #op1 = tf_mask_var(var1,vvTarget[i])\n",
    "                op1 = np.array(list(map(lambda x: 1 if x==vvTarget[i] else 0, var1.tolist())))\n",
    "                #op2 = tf_mask_var(var2,vvXi[j])\n",
    "                op2 = np.array(list(map(lambda x: 1 if x==vvXi[j] else 0, var2.tolist())))\n",
    "                #S[i][j] = tf_dot_mult(op1,op2)\n",
    "                S[i][j]=np.sum(op1*op2)\n",
    "                #print(S[i][j])\n",
    "        G2=0\n",
    "        N = np.sum(S)\n",
    "#         print(S)\n",
    "#         print(N)\n",
    "        Si =np.sum(S,axis=1)\n",
    "        Sj =np.sum(S,axis=0)\n",
    "        Df = ((len(vvTarget)-1)*(len(vvXi)-1))\n",
    "#         print(S[np.where(S>0)])\n",
    "        Dedf = len((S[np.where(S>0)]))\\\n",
    "                      - len(Si[np.where(Si>0)])\\\n",
    "                      - len(Sj[np.where(Sj>0)]) +1\n",
    "#         print(\"Dedf before if: \",Dedf)\n",
    "        if (Dedf<1):\n",
    "            Dedf=1\n",
    "            #print('Here is the trouble')\n",
    "            #return 0\n",
    "        #print(S)\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                #print(i)\n",
    "                #print(j)\n",
    "                if (S[i][j]>0):\n",
    "                    #print(Si[i])\n",
    "                    #print(Sj[j])\n",
    "                    G2 = G2 + S[i][j]*np.log((S[i][j])*N/(Si[i]*Sj[j]))\n",
    "                    #print(G2)\n",
    "        G2 = 2*G2\n",
    "        \n",
    "    else: # test conditional dependency\n",
    "        \n",
    "            \n",
    "        S = np.zeros((len(vvTarget),len(vvXi),szCondVars))\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                for k in range(0,szCondVars):\n",
    "                    condValue = GetCondValues(k,CondVars)\n",
    "                    #print(condValue)\n",
    "                    flagDataCondVars = np.ones(len(Xi['data']))\n",
    "                    for l in range(0,len(CondVars)):\n",
    "                        X_l = CondVars[l]\n",
    "                        flagDataCondVars = flagDataCondVars*np.array(list(map(lambda x: 1 if x==condValue[l] else 0, X_l['data'])))\n",
    "                        #op1 = tf_mask_var(X_l['data'],condValue[l])\n",
    "                        #flagDataCondVars = np.array(list(map(lambda x: 1 if x==condValue[l] else 0, X_l.tolist())))\n",
    "                        #flagDataCondVars = flagDataCondVars*op1\n",
    "                    var1=np.array(TargetNode['data'])\n",
    "                    var2=np.array(Xi['data'])\n",
    "                    # looking up for -1, 0 and 1\n",
    "                    op1 = np.array(list(map(lambda x: 1 if x==vvTarget[i] else 0, var1.tolist())))\n",
    "                    #op1 = tf_mask_var(var1,vvTarget[i])\n",
    "                    #print(op1)\n",
    "                    #op2 = tf_mask_var(var2,vvXi[j])\n",
    "                    op2 = np.array(list(map(lambda x: 1 if x==vvXi[j] else 0, var2.tolist())))\n",
    "                    #print(op2)\n",
    "                    #print(op1*op2*flagDataCondVars)\n",
    "                    S[i][j][k]=np.sum(op1*op2*flagDataCondVars)\n",
    "        G2 = 0\n",
    "        \n",
    "        Sjk = np.sum(S,axis=0)\n",
    "        Sik = np.sum(S,axis=1)\n",
    "        Sk = np.sum(Sjk,axis=0)\n",
    "        \n",
    "        #Sjk = sum(S,1);\n",
    "        #Sik = sum(S,2);\n",
    "        #Sk = sum(Sjk,2); \n",
    "        Dedf = len(S[np.where(S>0)]) -\\\n",
    "                len(Sik[np.where(Sik>0)]) -\\\n",
    "                len(Sjk[np.where(Sjk>0)]) +\\\n",
    "                len(Sk[np.where(Sk>0)])\n",
    "        if Dedf<1:\n",
    "            Dedf=1\n",
    "            #print('No, here is the trouble')\n",
    "            #return 0\n",
    "        \n",
    "        # compute G2 statistic\n",
    "        #print(S.shape)\n",
    "        #print(Sjk.shape)\n",
    "        #print(Sik.shape)\n",
    "        #print(Sk.shape)\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                for k in range(0,szCondVars):\n",
    "                    if (S[i][j][k]>0):\n",
    "                        #print(i,j,k)\n",
    "                        #test_op =  Sk[k]\n",
    "                        #test_op = Sik[i][k]\n",
    "                        #test_op = Sjk[j][k]\n",
    "                        G2=G2+(S[i][j][k] * np.log(\\\n",
    "                                    (S[i][j][k] * Sk[k])/\\\n",
    "                                    (Sik[i][k] * Sjk[j][k])))\n",
    "        G2 = 2*G2\n",
    "    \n",
    "    assoc = (alpha - (1 - chi2.cdf(G2,Dedf)))/alpha\n",
    "    #print(G2)\n",
    "    #print(Dedf)\n",
    "    #print(assoc)\n",
    "    if assoc<0:\n",
    "        assoc=0\n",
    "#     print(\"Target Node: \",TargetNode['name'])\n",
    "#     print(\"Node XI: \", Xi['name'])\n",
    "#     print(\"\\tG2:\",G2)\n",
    "#     print(\"\\tDedf: \",Dedf)\n",
    "#     print(\"\\tAssoc: \",assoc)\n",
    "#     print()\n",
    "#     print()\n",
    "    return assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getZsubset(bin_id,Z):\n",
    "    bin_str=str(bin_id[::-1])\n",
    "    Zsubset=[]\n",
    "    for i in range(0,len(bin_str)):\n",
    "        if bin_str[i]=='1':\n",
    "            Zsubset.append(Z[i])\n",
    "    return Zsubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrayUniverse(TargetNode,X):\n",
    "    Universe = []\n",
    "    for key in X:\n",
    "        #print(\"Key in X:\",key)\n",
    "        #print(\"Target Node is: \",TargetNode)\n",
    "        if (key!=TargetNode):\n",
    "            append_dict={'name':key,'data':X[key].copy(deep=True).tolist()}\n",
    "            Universe.append(append_dict)\n",
    "    return Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrayX(X):\n",
    "    returnable=[]\n",
    "    for key in X:\n",
    "        #tagged_c = MB_Column(X[key].copy(deep=True).tolist(),key)\n",
    "        append_dict={'name':key,'data':X[key].copy(deep=True).tolist()}\n",
    "        returnable.append(append_dict)\n",
    "    return returnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_names(dict_list):\n",
    "    print()\n",
    "    for item in dict_list:\n",
    "        print(item['name'],end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinAssoc(TargetNode, Xi,Z, fixedCondVars, X, alpha):\n",
    "    \n",
    "    min_assoc=999\n",
    "    #counter=0\n",
    "    #print(fixedCondVars)\n",
    "    if len(Z)==0:\n",
    "        #print(\"from minassoc1\")\n",
    "        #print(fixedCondVars)\n",
    "        min_assoc = Dep(TargetNode, Xi, fixedCondVars, X, alpha)\n",
    "        #counter+=1\n",
    "        subsetZ_min_assoc = fixedCondVars\n",
    "        #print(min_assoc)\n",
    "    else:\n",
    "        #print(2**len(Z)-1)\n",
    "        for IDsubsetZ_dec in range(0,2**len(Z)-1):\n",
    "            IDsubsetZ_bin = bin(IDsubsetZ_dec)\n",
    "            subsetZ = getZsubset(IDsubsetZ_bin,Z)\n",
    "            #print(len(Xi))\n",
    "            #print(\"from minassoc2\")\n",
    "            #print(IDsubsetZ_dec)\n",
    "            #print(subsetZ)\n",
    "            subsetZ_assoc=Dep(TargetNode, Xi, fixedCondVars+subsetZ,X,alpha)\n",
    "            #counter+=1\n",
    "            #print(subsetZ_assoc[IDsubsetZ_dec])\n",
    "            if subsetZ_assoc < min_assoc:\n",
    "                min_assoc = subsetZ_assoc\n",
    "                subsetZ_min_assoc = fixedCondVars + subsetZ\n",
    "                if (min_assoc==0):\n",
    "                    break\n",
    "    #print(\"Min Assoc module: \",counter)\n",
    "    return min_assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxMinHeuristic(TargetNode, CPC, Universe, X, alpha):\n",
    "    F=[]\n",
    "    assocF=-1\n",
    "    Z = CPC\n",
    "    fixedCondVars = []\n",
    "    if (len(CPC)>0):\n",
    "        Z = CPC[0:-1]           # all but the last one   \n",
    "        fixedCondVars = [CPC[-1]] # we use last one\n",
    "    \n",
    "    for i in range(len(Universe)-1,-1,-1):\n",
    "    #for i in range(0,len(Universe)):\n",
    "        if (len(Universe[i])==0):\n",
    "            continue\n",
    "        minAssoc = MinAssoc(TargetNode,Universe[i], Z,fixedCondVars, X,alpha)\n",
    "        #print(\"\\nFor \"+Universe[i]['name']+\"the minAssoc value was: \"+str(minAssoc))\n",
    "        if minAssoc <= 0.0:\n",
    "            #Universe[i]=[]\n",
    "            Universe.pop(i)\n",
    "            continue\n",
    "        if minAssoc>assocF:\n",
    "            assocF = minAssoc\n",
    "            F=Universe[i]\n",
    "        \n",
    "        #print(F)\n",
    "    return [F,assocF,Universe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_CPC(CPC):\n",
    "    for i in CPC:\n",
    "        print(i['name']+\" \", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMPC_(TargetNode,Universe,X,alpha):\n",
    "    \n",
    "    CPC=[]\n",
    "    #print(TargetNode)\n",
    "    # Phase I: Foward\n",
    "    print(\"Entering Phase I\")\n",
    "    print(\"MMPC_beggining: \\n\"+str(len(Universe)))\n",
    "    while len(Universe)>0:\n",
    "        CPC_old = list(CPC) # copy\n",
    "        maxminheur=MaxMinHeuristic(TargetNode,CPC,list(Universe),X,alpha)\n",
    "        # maxminheur = [F, assocF, Universe]\n",
    "        F = maxminheur[0]\n",
    "        assocF = maxminheur[1]\n",
    "        #print(assocF)\n",
    "        Universe = maxminheur[2]\n",
    "        #print(\"Universe printing line\")\n",
    "        #print(Universe)\n",
    "        #print(\"\\n=============================\")\n",
    "        #print(CPC)\n",
    "        if assocF > 0:\n",
    "            CPC.append(F)\n",
    "            indF=Universe.index(F)\n",
    "            Universe.pop(indF)\n",
    "        #if (len(CPC)==len(CPC_old)) or (len(CPC)>0.3*(len(Universe)-1)):\n",
    "        if (len(CPC)==len(CPC_old)):\n",
    "            break\n",
    "#         print(\"Universe actual size:\")\n",
    "#         print(len(Universe))\n",
    "#         print(\"CPC actual size:\")\n",
    "#         print(len(CPC))\n",
    "#         print(\"CPC contents:\")\n",
    "#         print_CPC(CPC)\n",
    "    \n",
    "    # Phase 2: Backward\n",
    "    print(\"\\nExiting MMPC_\")\n",
    "#     CPC=CPC[::-1]\n",
    "#     if len(CPC)>1:\n",
    "#         Z=list(CPC)\n",
    "#         for i in range(len(CPC)-1,-1,-1):\n",
    "#             # index is i\n",
    "#             Z.pop(i)\n",
    "#             #print(\"Analyzing D-separator for \",CPC[i]['name'])\n",
    "#             if ExistDseparator(TargetNode,CPC[i],Z,X,alpha)[0] == True:\n",
    "#                 #print(\"it did exist! removing from cpc.\")\n",
    "#                 CPC.pop(i)\n",
    "#     return symmetry_test_v2(TargetNode,CPC,X,alpha)\n",
    "    return CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMPC(TargetNode, X, alpha):\n",
    "    # The universe will be an array of DataFrame Columns\n",
    "    Universe = arrayUniverse(TargetNode,X)\n",
    "    print(len(Universe))\n",
    "    X = arrayX(X)\n",
    "    print(len(X))\n",
    "    for column in X:\n",
    "        if (column['name']==TargetNode):\n",
    "            TargetNode = column\n",
    "            break\n",
    "    CPC = MMPC_(TargetNode,Universe,X,alpha)\n",
    "    return CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMPC_Parallel(arg):\n",
    "    TargetNode, X, alpha = arg\n",
    "    Universe = arrayUniverse(TargetNode,X)\n",
    "    print(len(Universe))\n",
    "    X = arrayX(X)\n",
    "    print(len(X))\n",
    "    for column in X:\n",
    "        if (column['name']==TargetNode):\n",
    "            TargetNode = column\n",
    "            break\n",
    "    CPC = MMPC_(TargetNode,Universe,X,alpha)\n",
    "    return CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeysPartition(targetNode, data_keys,n_partitions):\n",
    "    n_partitions = int(np.log2(len(data_keys)))*2*2  # log2 ( number of variables ) divided by 2\n",
    "    partitions = []\n",
    "    partition_diff = int(int(len(data_keys)/n_partitions)/2)\n",
    "    for i in range(0,n_partitions):\n",
    "        if (i==0):\n",
    "            partitions.append(list(data_keys[-partition_diff::])+list(data_keys[0:int(len(data_keys)/n_partitions)]))\n",
    "            #partitions.append(list(data_keys[0:int(len(data_keys)/n_partitions)]))\n",
    "            continue\n",
    "        if (i<n_partitions-1):\n",
    "            partitions.append(list(data_keys[-partition_diff+int(len(data_keys)/n_partitions)*i:int(len(data_keys)/n_partitions)*(i+1)]))\n",
    "            #partitions.append(list(data_keys[int(len(data_keys)/n_partitions)*i:int(len(data_keys)/n_partitions)*(i+1)]))\n",
    "            continue\n",
    "        if (i==n_partitions-1):\n",
    "            partitions.append(list(data_keys[-partition_diff+int(len(data_keys)/n_partitions)*i::]))\n",
    "            #partitions.append(list(data_keys[int(len(data_keys)/n_partitions)*i::]))\n",
    "            \n",
    "    for i in range(0,n_partitions):\n",
    "        for j in range(len(partitions[i])-1,-1,-1):\n",
    "            if (partitions[i][j]==targetNode):\n",
    "                partitions[i].pop(j)\n",
    "        partitions[i] = partitions[i] + [targetNode]\n",
    "    return partitions,n_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=\"Hello, I love you, Won't you tell me your name\"\n",
    "abc.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alpha=0.05\n",
    "print(\"======================================================================================\")\n",
    "print(\"Partitioned MMPC_ Algorithm\")\n",
    "print(\"======================================================================================\")\n",
    "main_directory= '/home/a20114261/alarm_datasets/Alarm10/'\n",
    "if not os.path.exists(main_directory):\n",
    "    print(\"Bad routing.\")\n",
    "PCs=[]\n",
    "for filename in os.listdir(main_directory):\n",
    "    print(\"Analizing \"+filename)\n",
    "    X = pd.read_csv(main_directory+'/'+filename,delimiter='  ',header=None)\n",
    "    x_heads = []\n",
    "    for column in X:\n",
    "        X[column] = list(map(lambda x: x , X[column]))\n",
    "    for i in range(0,len(X.keys())):\n",
    "        max_v=1\n",
    "        for row in X[X.keys()[i]]:\n",
    "            if (row>max_v-1):\n",
    "                max_v+=row-(max_v-1)\n",
    "        save_i=i+1\n",
    "        x_heads.append(\"Node\"+str(save_i)+\"@\"+str(max_v))\n",
    "    X.columns = x_heads\n",
    "    data_c_size = len(X[X.keys()[1]])\n",
    "    data_vars = [0,1,2,3]\n",
    "    print(\"======================================================================================\")\n",
    "    \n",
    "    # selection of target\n",
    "    for TargetNodeSelected in [x for x in X.keys() if int(x.split('@')[1]) == 2]:\n",
    "    #  -----------------\n",
    "        PC=[]\n",
    "        n_partitions = 0\n",
    "        keys_partitions, n_partitions = getKeysPartition(TargetNodeSelected,X.keys(),n_partitions)\n",
    "        arg_instances = []\n",
    "        for i in range(0,n_partitions):\n",
    "            arg_instances.append((TargetNodeSelected,X[keys_partitions[i]],alpha))\n",
    "        PC=Parallel(n_jobs=n_partitions)(map(delayed(MMPC_Parallel),arg_instances))\n",
    "        PC_dict={}\n",
    "        PC_dict['TargetNode']=TargetNodeSelected\n",
    "        PC_dict['pcs']=PC\n",
    "        PC_dict['filename']=filename\n",
    "        PCs.append(PC_dict)\n",
    "# for after\n",
    "n_parallel_forks = n_partitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_universe=[]\n",
    "for PC_dict in PCs:\n",
    "    PC_train={}\n",
    "    PC_train['TargetNode']=PC_dict['TargetNode']\n",
    "    PC_train['dataset']=PC_dict['filename']\n",
    "    PC_train['supercpc']=[]\n",
    "    for pc_subset in PC_dict['pcs']:\n",
    "        if (len(pc_subset)==0):\n",
    "            continue\n",
    "        for e in pc_subset:\n",
    "            PC_train['supercpc'].append(e['name'].split('@')[0])\n",
    "    if (len(PC_train['supercpc'])==0):\n",
    "        continue\n",
    "    PC_train['supercpc']=list(set(PC_train['supercpc']))\n",
    "    initial_universe.append(PC_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging results\n",
    "from operator import itemgetter\n",
    "log_directory=\"/home/a20114261/alarm_datasets/log_results/mmpc_partitioned\"\n",
    "if not os.path.exists(log_directory):\n",
    "    os.makedirs(log_directory)\n",
    "ff = open(log_directory+'/supercpc_log.txt','w')\n",
    "for e in sorted(initial_universe, key=itemgetter('dataset'), reverse=True):\n",
    "    ff.write(e['dataset']+';')\n",
    "    ff.write(e['TargetNode']+';')\n",
    "    for node in e['supercpc']:\n",
    "        ff.write(node+'_')\n",
    "    ff.write('\\n')\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info recovery\n",
    "recovery_proof=[]\n",
    "ff = open(log_directory+'/supercpc_log.txt','r')\n",
    "recovery_read = ff.read().split('\\n')\n",
    "for line in recovery_read:\n",
    "    sub_line=line.split(';')\n",
    "    if (len(sub_line)<3):\n",
    "        continue\n",
    "    p_dict={}\n",
    "    p_dict['dataset']=sub_line[0]\n",
    "    p_dict['TargetNode']=sub_line[1]\n",
    "    p_dict['supercppc']=sub_line[2].split('_')\n",
    "    recovery_proof.append(p_dict)\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_proof[0]['dataset'].split('_')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_heads_train=[]\n",
    "for e in x_heads:\n",
    "    x_heads_train.append(e.split('@')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "import time\n",
    "PC_mean_score=[]\n",
    "for filename in os.listdir(main_directory):\n",
    "    for PC_set in [x for x in initial_universe if x['dataset']==filename]:    \n",
    "        start = time.time()\n",
    "        print(\"For \"+PC_set['dataset']+\" and Target Node \"+PC_set['TargetNode'])\n",
    "        print(\"CPC of length \"+str(len(PC_set['supercpc'])))\n",
    "        PC_mean_score.append([filename,CandidateScore(PC_set,PC_set['TargetNode'].split('@')[0],i)])    \n",
    "        end = time.time()\n",
    "        #print(\"Time elapsed: \"+str(end - start)+\" seconds\")\n",
    "        print()\n",
    "        print(\"=================================================================\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging scored results\n",
    "ff = open(log_directory+'/scored_supercpc_log.txt','w')\n",
    "for e in sorted(PC_mean_score, key=itemgetter(0),reverse=True):\n",
    "    ff.write(e[0]+';')\n",
    "    ff.write(e[1]['TargetNode']+';')\n",
    "    for node in e[1]['supercpc']:\n",
    "        ff.write(node+'_')\n",
    "    ff.write(';')\n",
    "    for class_pred in e[1]['class_precisions']:\n",
    "            ff.write(class_pred[1]+'_')\n",
    "            for arr_acc in class_pred[0]:\n",
    "                ff.write(str(arr_acc[0])+'\\t')\n",
    "                ff.write(str(arr_acc[1])+'\\t')\n",
    "                ff.write(str(arr_acc[2])+'\\t')\n",
    "            ff.write('_')\n",
    "    ff.write('\\n')\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recovering results\n",
    "ff = open(log_directory+'/scored_supercpc_log.txt','r')\n",
    "#line_rec_scor =  ff.read().split('\\n')[0].split(';')\n",
    "recovery_scored_supercpc=[]\n",
    "for line in ff.read().split('\\n'):\n",
    "    line_arr=[]\n",
    "    line_rec_scor=line.split(';')\n",
    "    node_dict={}\n",
    "    if (len(line_rec_scor[0])==0):\n",
    "        continue\n",
    "    node_dict['TargetNode']=line_rec_scor[1]\n",
    "    node_dict['supercpc']=line_rec_scor[2].split('_')\n",
    "    node_dict['class_precisions']=[]\n",
    "    line_arr.append(line_rec_scor[0])\n",
    "    \n",
    "    for i in range(0,len(line_rec_scor[3].split('_')),4):\n",
    "        if (len(line_rec_scor[3].split('_')[i:i+3][0])==0):\n",
    "            continue\n",
    "        acc_arr=[]\n",
    "        acc_arr.append([float(x) for x in line_rec_scor[3].split('_')[i+3].split('\\t')[:3]])\n",
    "        acc_arr.append([float(x) for x in line_rec_scor[3].split('_')[i+3].split('\\t')[3:6]])\n",
    "        node_dict['class_precisions'].append([acc_arr,line_rec_scor[3].split('_')[i]+'_'+line_rec_scor[3].split('_')[i+1]+'_'+line_rec_scor[3].split('_')[i+2]])\n",
    "        \n",
    "    line_arr.append(node_dict)\n",
    "    recovery_scored_supercpc.append(line_arr)\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recovery_scored_supercpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in recovery_scored_supercpc:\n",
    "    print(\"\\t* Training dataset: \"+e[0])\n",
    "    #print(\"\\t- Training dataset: \"+e[1]['dataset'])\n",
    "    print(\"\\t- Target Node: \"+e[1]['TargetNode'])\n",
    "    print(\"\\t- Supercpc of length: \"+str(len(e[1]['supercpc'])))\n",
    "    print(\"\\t- Tests results:\")\n",
    "    for i in e[1]['class_precisions']:\n",
    "        print(\"\\t\\t- \",end=' ')\n",
    "        #print(i[1])\n",
    "        print(\"Test dataset: \"+i[1]+\"\\t G-mean: \"+str(g_mean(i[0])))\n",
    "        #print(\"\\t\\t> \",end=' ')\n",
    "        for j in i[0]:\n",
    "            print(\"\\t\\t   >>>>\\t\",end='')\n",
    "            print(\"Class: \"+str(j[0]))\n",
    "            print(\"\\t\\t\\t\"+\"Samples: \"+str(j[1]))\n",
    "            print(\"\\t\\t\\t\"+\"Class precision: \"+str(j[2]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = open(log_directory+'/alarm10_binary_nodes_pc_size.txt','w')\n",
    "\n",
    "for e in PC_mean_score:\n",
    "    ff.write(e[0]+';')\n",
    "    ff.write(e[1]['TargetNode']+';')\n",
    "    ff.write(str(len(e[1]['supercpc']))+'\\n')\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in balance_ranking[:10]:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in balance_ranking_2[:10]:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in balance_ranking_3[:10]:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "mylist = [['a',1,2],['d',3,4]]\n",
    "math.ceil([j for i in balance_ranking for j in i].index('Node104@2')/3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging nodes selected for other feature selection methods\n",
    "ff = open(log_directory+'/alarm10_binary_nodes_selected.txt','w')\n",
    "for e in balance_ranking:\n",
    "    ff.write(e[0]+';')\n",
    "    ff.write(str(e[1])+'\\n')\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "balance_ratio_source = [x for x in PC_mean_score\n",
    "                        if x[0].split('_')[1] == 's5000']\n",
    "evaluated_nodes = []\n",
    "balance_ranking = []\n",
    "bl_sorted = sorted(balance_ratio_source, key=lambda x: x[1]['TargetNode'])\n",
    "for e in bl_sorted:\n",
    "    if e[1]['TargetNode'] in evaluated_nodes:\n",
    "        continue\n",
    "    evaluated_nodes.append(e[1]['TargetNode'])\n",
    "    print(e[1]['TargetNode'])\n",
    "    b_rank = []\n",
    "    g_rank = []\n",
    "    for sub_e in [x for x in bl_sorted if x[1]['TargetNode'] == e[1]['TargetNode']]:\n",
    "        for prec in sub_e[1]['class_precisions']:\n",
    "            b_rank.append(balance_scoring(prec[0][0][1],prec[0][1][1]))\n",
    "            g_rank.append(g_mean(prec[0]))\n",
    "    balance_ranking.append([e[1]['TargetNode'],sum(b_rank)/len(b_rank),sum(g_rank)/len(g_rank)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the evaluated_nodes list to evaluate s1000\n",
    "balance_ratio_source_2 = [x for x in PC_mean_score if x[0].split('_')[1] == 's1000']\n",
    "evaluated_nodes_2 = []\n",
    "balance_ranking_2 = []\n",
    "bl_sorted_2 = sorted(balance_ratio_source_2, key=lambda x: x[1]['TargetNode'])\n",
    "for e in bl_sorted_2:\n",
    "    if e[1]['TargetNode'] not in evaluated_nodes:\n",
    "            continue\n",
    "    if e[1]['TargetNode'] in evaluated_nodes_2:\n",
    "            continue\n",
    "    evaluated_nodes_2.append(e[1]['TargetNode'])\n",
    "    print(e[1]['TargetNode'])\n",
    "    #b_rank = []\n",
    "    g_rank = []\n",
    "    for sub_e in [x for x in bl_sorted_2 if x[1]['TargetNode'] == e[1]['TargetNode']]:\n",
    "        for prec in sub_e[1]['class_precisions']:\n",
    "            g_rank.append(g_mean(prec[0]))\n",
    "    balance_ranking_2.append([e[1]['TargetNode'],\n",
    "                              balance_ranking[math.ceil([j for i in balance_ranking for j in i].index(e[1]['TargetNode'])/3.0)][1],\n",
    "                              sum(g_rank)/len(g_rank)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the evaluated_nodes list to evaluate s500\n",
    "balance_ratio_source_3 = [x for x in PC_mean_score if x[0].split('_')[1] == 's500']\n",
    "evaluated_nodes_3 = []\n",
    "balance_ranking_3 = []\n",
    "bl_sorted_3 = sorted(balance_ratio_source_3, key=lambda x: x[1]['TargetNode'])\n",
    "for e in bl_sorted_3:\n",
    "    if e[1]['TargetNode'] not in evaluated_nodes:\n",
    "            continue\n",
    "    if e[1]['TargetNode'] in evaluated_nodes_3:\n",
    "            continue\n",
    "    evaluated_nodes_3.append(e[1]['TargetNode'])\n",
    "    print(e[1]['TargetNode'])\n",
    "    #b_rank = []\n",
    "    g_rank = []\n",
    "    for sub_e in [x for x in bl_sorted_3 if x[1]['TargetNode'] == e[1]['TargetNode']]:\n",
    "        for prec in sub_e[1]['class_precisions']:\n",
    "            g_rank.append(g_mean(prec[0]))\n",
    "    balance_ranking_3.append([e[1]['TargetNode'],\n",
    "                              balance_ranking[math.ceil([j for i in balance_ranking for j in i].index(e[1]['TargetNode'])/3.0)][1],\n",
    "                              sum(g_rank)/len(g_rank)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0.0,0.6,0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\"\"\"\n",
    "Plotting a three-way ANOVA\n",
    "==========================\n",
    "\n",
    "_thumb: .42, .5\n",
    "\"\"\"\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Creating the df\n",
    "df = pd.DataFrame(columns=[\"Balance Rank\", \"G-mean\",\"Feature Selection Method\",\"Number of samples\"])\n",
    "\n",
    "# sumarizing data for intervals of 0.25\n",
    "sumarized_balance_rank=[]\n",
    "sumarized_balance_rank_2=[]\n",
    "sumarized_balance_rank_3=[]\n",
    "ticks_step=0.25\n",
    "for balance_ticks in np.arange(0.0,0.75,ticks_step):\n",
    "    sumarized_balance_rank.append([x[2] for x in balance_ranking if x[1]>=balance_ticks and x[1]<balance_ticks+ticks_step])\n",
    "    sumarized_balance_rank_2.append([x[2] for x in balance_ranking_2 if x[1]>=balance_ticks and x[1]<balance_ticks+ticks_step])\n",
    "    sumarized_balance_rank_3.append([x[2] for x in balance_ranking_3 if x[1]>=balance_ticks and x[1]<balance_ticks+ticks_step])\n",
    "        \n",
    "#     sumarized_balance_rank.append(sum([x[2] for x in balance_ranking if x[1]>=balance_ticks and x[1]<balance_ticks+ticks_step])\\\n",
    "#                                   /len([x[2] for x in balance_ranking if x[1]>=balance_ticks and x[1]<balance_ticks+ticks_step]))\n",
    "#     sumarized_balance_rank_2.append(sum([x[2] for x in balance_ranking_2 if x[1]>=balance_ticks and x[1]<balance_ticks+ticks_step])\\\n",
    "#                                   /len([x[2] for x in balance_ranking_2 if x[1]>=balance_ticks and x[1]<balance_ticks+ticks_step]))\n",
    "#     sumarized_balance_rank_3.append(sum([x[2] for x in balance_ranking_3 if x[1]>=balance_ticks and x[1]<balance_ticks+ticks_step])\\\n",
    "#                                   /len([x[2] for x in balance_ranking_3 if x[1]>=balance_ticks and x[1]<balance_ticks+ticks_step]))\n",
    "\n",
    "\n",
    "for e in sumarized_balance_rank:\n",
    "    for sub_e in e:\n",
    "        df = df.append({\n",
    "             \"Balance Rank\":str(math.ceil(100*sumarized_balance_rank.index(e)*ticks_step)/100) +\n",
    "                                ' - ' + \n",
    "                            str(math.ceil(100*(sumarized_balance_rank.index(e)*ticks_step+ticks_step))/100), \n",
    "            \"G-mean\":sub_e,\n",
    "            \"Feature Selection Method\":'MMPC_Partitioned_5000',\n",
    "            \"Number of samples\":5000\n",
    "              }, ignore_index=True)\n",
    "\n",
    "for e in sumarized_balance_rank_2:\n",
    "    for sub_e in e:\n",
    "        df = df.append({\n",
    "             \"Balance Rank\":str(math.ceil(100*sumarized_balance_rank_2.index(e)*ticks_step)/100) +\n",
    "                            ' - ' + \n",
    "                            str(math.ceil(100*(sumarized_balance_rank_2.index(e)*ticks_step+ticks_step))/100), \n",
    "            \"G-mean\":sub_e,\n",
    "            \"Feature Selection Method\":'MMPC_Partitioned_1000',\n",
    "            \"Number of samples\":5000\n",
    "              }, ignore_index=True)\n",
    "    \n",
    "for e in sumarized_balance_rank_3:\n",
    "    for sub_e in e:\n",
    "        df = df.append({\n",
    "             \"Balance Rank\":str(math.ceil(100*sumarized_balance_rank_3.index(e)*ticks_step)/100) +\n",
    "                            ' - ' + \n",
    "                            str(math.ceil(100*(sumarized_balance_rank_3.index(e)*ticks_step+ticks_step))/100), \n",
    "            \"G-mean\":sub_e,\n",
    "            \"Feature Selection Method\":'MMPC_Partitioned_500',\n",
    "            \"Number of samples\":5000\n",
    "              }, ignore_index=True)\n",
    "\n",
    "\n",
    "# Draw a pointplot to show pulse as a function of three categorical factors\n",
    "sns.set_color_codes(\"pastel\")\n",
    "g = sns.factorplot(x=\"Balance Rank\", y=\"G-mean\", hue=\"Feature Selection Method\",\n",
    "                   col=\"Number of samples\", data=df,\n",
    "                   capsize=.2, palette=\"YlGnBu_d\", size=7, aspect=1.5)\n",
    "g.despine(left=True)\n",
    "\n",
    "g.set(yticks=np.arange(0.0,1.0,0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.savefig(\"/home/a20114261/GeneInteractions/GeneInteractionsBN/FeatureSelection/MMPC_Partitioned_graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in sumarized_balance_rank:\n",
    "    print(e)\n",
    "for e in sumarized_balance_rank_2:\n",
    "    print(e)\n",
    "for e in sumarized_balance_rank_3:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%pycodestyle\n",
    "def balance_scoring(size1, size2):\n",
    "    f_size1 = float(size1)\n",
    "    f_size2 = float(size2)\n",
    "    if (f_size1/f_size2 <= 1):\n",
    "        return f_size1/f_size2\n",
    "    else:\n",
    "        return f_size2/f_size1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CandidateScore(PobDict,TargetEvalNode,intercept_scaling):\n",
    "    global x_heads_train\n",
    "\n",
    "    # training and scoring\n",
    "    X_train_df = pd.read_csv(main_directory+PobDict['dataset'],delimiter='  ',header=None)\n",
    "    X_train_df.columns=x_heads_train\n",
    "    \n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(X_train_df, X_train_df[TargetEvalNode], test_size=0.1, random_state=0)\n",
    "    \n",
    "    X_train = X_train_df\n",
    "    Y_train = X_train_df[TargetEvalNode]\n",
    "    \n",
    "    clf = LinearSVC()\n",
    "    clf.fit(X_train[PobDict['supercpc']],Y_train)\n",
    "    \n",
    "    PobDict['class_precisions']=[]\n",
    "    for filename in [x for x in os.listdir(main_directory) if x.split('_')[1]==PobDict['dataset'].split('_')[1]]:\n",
    "        print(len([x for x in os.listdir(main_directory) if x.split('_')[1]==PobDict['dataset'].split('_')[1]]))\n",
    "        if (filename == PobDict['dataset']):\n",
    "            continue\n",
    "        print(\"*****\")\n",
    "        print()\n",
    "        print(\"Testing on dataset: \"+filename)\n",
    "        X_test = pd.read_csv(main_directory+filename,delimiter='  ',header=None)\n",
    "        X_test.columns = x_heads_train\n",
    "        \n",
    "\n",
    "        # get score values\n",
    "        precision = []\n",
    "        values_counter=set(X_train_df[TargetEvalNode])\n",
    "        for val in values_counter:\n",
    "            #Y_pred_c = clf.predict(X_train_df.query( TargetEvalNode+'== '+str(i))[PobDict['supercpc']][-100:])\n",
    "            if (len(X_test.query( TargetEvalNode+'== '+str(val)))==0):\n",
    "                precision.append([val,0,0])\n",
    "                continue\n",
    "            Y_pred_c = clf.predict(X_test.query( TargetEvalNode+'== '+str(val))[PobDict['supercpc']])\n",
    "            precision.append([val,len(Y_pred_c),Y_pred_c.tolist().count(val)/len(Y_pred_c)])\n",
    "\n",
    "        for e in range(0,len(precision)):\n",
    "            print('precision for class '+str(precision[e][0])+' with '+str(precision[e][1])+' samples in dataset : '+str(precision[e][2]))\n",
    "    \n",
    "        print(\"G-mean score: \"+str(g_mean(precision)))\n",
    "        print(\"*****\")\n",
    "        print()\n",
    "        #print(clf.score(X_test[PobDict['supercpc'][:10]],Y_test))\n",
    "        PobDict['class_precisions'].append([precision,filename])\n",
    "    return PobDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_mean(precision_arr):\n",
    "    counter=1\n",
    "    for e in precision_arr:\n",
    "        counter=counter*e[2]\n",
    "    \n",
    "    return counter**(1/len(precision_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
