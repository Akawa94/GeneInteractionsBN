{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.stats import chi2\n",
    "from math import floor\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support as full_score\n",
    "import operator\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_mean(tp_0,tp_1):\n",
    "    return (tp_0*tp_1)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"/home/a20114261/GeneInteractions/GeneInteractionsBN_Datasets/DataGeneCausality/Tt/Erk.csv\",sep=\";\").query('(Pvalue>=0.5) | (Pvalue<=0.01)')\n",
    "del X_test['CauseGene']\n",
    "del X_test['EffectGene']\n",
    "del X_test['Replicate']\n",
    "del X_test['Treatment']\n",
    "X_test['Pvalue']=list(map(lambda x: 1 if x<=0.01 else 0, X_test['Pvalue']))\n",
    "for column in X_test:\n",
    "    if (column != \"Pvalue\"):\n",
    "        X_test[column] = list(map(lambda x: x+1 , X_test[column]))\n",
    "x_test_heads = []\n",
    "for i in range(0,len(X_test.keys())):\n",
    "    max_v=1\n",
    "    for row in X_test[X_test.keys()[i]]:\n",
    "        if (row>max_v-1):\n",
    "            max_v+=row-(max_v-1)\n",
    "    save_i=i+1\n",
    "    x_test_heads.append(\"Node\"+str(save_i)+\"@\"+str(max_v))\n",
    "X_test.columns = x_test_heads\n",
    "Y_test = X_test[X_test.keys()[0]]\n",
    "TargetNode =X_test.keys()[0]\n",
    "x_heads=x_test_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "X_train=np.ndarray((0,len(X_test.keys())-1))\n",
    "Y_train=np.ndarray((0,1))\n",
    "for filename in os.listdir(main_directory):\n",
    "#     if (filename!='batch_1.csv'):\n",
    "#         continue\n",
    "    X = pd.read_csv(main_directory+'/'+filename)\n",
    "    # each cell can be -1, 0 or 1\n",
    "    #Pre-processing data\n",
    "    #X['Pvalue']=list(map(lambda x: 1 if x<=0.01 else (0 if x>=0.5 else -1), X['Pvalue']))\n",
    "    #print(main_directory+filename)\n",
    "    X['Pvalue']=list(map(lambda x: 1 if x<=0.05 else 0, X['Pvalue']))\n",
    "    del X['Unnamed: 0']\n",
    "    del X['TrueIndex']\n",
    "    del X['CauseGene']\n",
    "    del X['EffectGene']\n",
    "    del X['Replicate']\n",
    "    del X['Treatment']\n",
    "    for column in X:\n",
    "        if (column != \"Pvalue\"):\n",
    "            X[column] = list(map(lambda x: x+1 , X[column]))\n",
    "    X.columns = x_heads        \n",
    "    Y_train=np.append(X.as_matrix([TargetNode]),Y_train,axis=0)\n",
    "    #print(set(X['Node1@2']))\n",
    "    del X[TargetNode]\n",
    "    print(len(X.keys()))\n",
    "    X_train=np.append(X.as_matrix(X.keys()),X_train,axis=0)\n",
    "    x_train_heads = X.keys()\n",
    "\n",
    "# making the new dataframe\n",
    "X_train_df = pd.DataFrame(X_train,columns=x_train_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExistDseparator(TargetNode,Xi, Z, X, alpha):\n",
    "    flagExist = False\n",
    "    dsepSet=[]\n",
    "    #counter=0\n",
    "    #print_names(Z)\n",
    "    for i in range(0,(2**len(Z))-1):\n",
    "        IDsubsetZ_dec = i\n",
    "        IDsubsetZ_bin = bin(IDsubsetZ_dec)\n",
    "        subsetZ = getZsubset(IDsubsetZ_bin,Z)\n",
    "        # no cache\n",
    "        #print(\"from exist dseparator\")\n",
    "        dep = Dep(TargetNode,Xi,subsetZ, X, alpha)\n",
    "        #print(subsetZ)\n",
    "        #print(dep)\n",
    "        if (dep==0):\n",
    "            flagExist = True\n",
    "            dsepSet = subsetZ\n",
    "            break\n",
    "    #print(\"Module exist d-separator: \",counter)\n",
    "    return [flagExist,dsepSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCondValues(z, condVars):\n",
    "    if len(condVars)>1:\n",
    "        maxz = reduce(operator.mul,[int(x['name'].split('@')[1]) for x in condVars],1)\n",
    "        d = np.zeros(len(condVars))\n",
    "        \n",
    "        div_dim = maxz/int(condVars[0]['name'].split('@')[1])\n",
    "        num2div = z\n",
    "        for i in range(0,len(condVars)-1):\n",
    "            d[i] = floor(num2div/div_dim)\n",
    "            num2div=num2div%div_dim\n",
    "            div_dim = div_dim/int(condVars[i+1]['name'].split('@')[1])\n",
    "        d[i+1] = num2div\n",
    "    else:\n",
    "        d = [z]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dep(TargetNode, Xi, CondVars, X, alpha):\n",
    "    #print('in Dep')\n",
    "    global data_c_size\n",
    "    \n",
    "    vvTarget = []\n",
    "    for i in range(int(TargetNode['name'].split('@')[1])):\n",
    "        vvTarget.append(i)\n",
    "    \n",
    "    vvXi = []\n",
    "    for i in range(int(Xi['name'].split('@')[1])):\n",
    "        vvXi.append(i)\n",
    "    \n",
    "    szCondVars = 1\n",
    "    #print(CondVars)\n",
    "    for column in CondVars:\n",
    "        #print(column)\n",
    "        szCondVars *= int(column['name'].split('@')[1])\n",
    "    \n",
    "    if data_c_size <= (5 * len(vvTarget)*len(vvXi)*(szCondVars)):\n",
    "        return 1\n",
    "    \n",
    "    if (len(CondVars)==0):\n",
    "        S = np.zeros((len(vvTarget),len(vvXi)))\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                var1=np.array(TargetNode['data']) # Target Node\n",
    "                var2=np.array(Xi['data'])\n",
    "                # looking up for 0, 1 and 2\n",
    "                #op1 = tf_mask_var(var1,vvTarget[i])\n",
    "                op1 = np.array(list(map(lambda x: 1 if x==vvTarget[i] else 0, var1.tolist())))\n",
    "                #op2 = tf_mask_var(var2,vvXi[j])\n",
    "                op2 = np.array(list(map(lambda x: 1 if x==vvXi[j] else 0, var2.tolist())))\n",
    "                #S[i][j] = tf_dot_mult(op1,op2)\n",
    "                S[i][j]=np.sum(op1*op2)\n",
    "                #print(S[i][j])\n",
    "        G2=0\n",
    "        N = np.sum(S)\n",
    "#         print(S)\n",
    "#         print(N)\n",
    "        Si =np.sum(S,axis=1)\n",
    "        Sj =np.sum(S,axis=0)\n",
    "        Df = ((len(vvTarget)-1)*(len(vvXi)-1))\n",
    "#         print(S[np.where(S>0)])\n",
    "        Dedf = len((S[np.where(S>0)]))\\\n",
    "                      - len(Si[np.where(Si>0)])\\\n",
    "                      - len(Sj[np.where(Sj>0)]) +1\n",
    "#         print(\"Dedf before if: \",Dedf)\n",
    "        if (Dedf<1):\n",
    "            Dedf=1\n",
    "            #print('Here is the trouble')\n",
    "            #return 0\n",
    "        #print(S)\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                #print(i)\n",
    "                #print(j)\n",
    "                if (S[i][j]>0):\n",
    "                    #print(Si[i])\n",
    "                    #print(Sj[j])\n",
    "                    G2 = G2 + S[i][j]*np.log((S[i][j])*N/(Si[i]*Sj[j]))\n",
    "                    #print(G2)\n",
    "        G2 = 2*G2\n",
    "        \n",
    "    else: # test conditional dependency\n",
    "        \n",
    "            \n",
    "        S = np.zeros((len(vvTarget),len(vvXi),szCondVars))\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                for k in range(0,szCondVars):\n",
    "                    condValue = GetCondValues(k,CondVars)\n",
    "                    #print(condValue)\n",
    "                    flagDataCondVars = np.ones(len(Xi['data']))\n",
    "                    for l in range(0,len(CondVars)):\n",
    "                        X_l = CondVars[l]\n",
    "                        flagDataCondVars = flagDataCondVars*np.array(list(map(lambda x: 1 if x==condValue[l] else 0, X_l['data'])))\n",
    "                        #op1 = tf_mask_var(X_l['data'],condValue[l])\n",
    "                        #flagDataCondVars = np.array(list(map(lambda x: 1 if x==condValue[l] else 0, X_l.tolist())))\n",
    "                        #flagDataCondVars = flagDataCondVars*op1\n",
    "                    var1=np.array(TargetNode['data'])\n",
    "                    var2=np.array(Xi['data'])\n",
    "                    # looking up for -1, 0 and 1\n",
    "                    op1 = np.array(list(map(lambda x: 1 if x==vvTarget[i] else 0, var1.tolist())))\n",
    "                    #op1 = tf_mask_var(var1,vvTarget[i])\n",
    "                    #print(op1)\n",
    "                    #op2 = tf_mask_var(var2,vvXi[j])\n",
    "                    op2 = np.array(list(map(lambda x: 1 if x==vvXi[j] else 0, var2.tolist())))\n",
    "                    #print(op2)\n",
    "                    #print(op1*op2*flagDataCondVars)\n",
    "                    S[i][j][k]=np.sum(op1*op2*flagDataCondVars)\n",
    "        G2 = 0\n",
    "        \n",
    "        Sjk = np.sum(S,axis=0)\n",
    "        Sik = np.sum(S,axis=1)\n",
    "        Sk = np.sum(Sjk,axis=0)\n",
    "        \n",
    "        #Sjk = sum(S,1);\n",
    "        #Sik = sum(S,2);\n",
    "        #Sk = sum(Sjk,2); \n",
    "        Dedf = len(S[np.where(S>0)]) -\\\n",
    "                len(Sik[np.where(Sik>0)]) -\\\n",
    "                len(Sjk[np.where(Sjk>0)]) +\\\n",
    "                len(Sk[np.where(Sk>0)])\n",
    "        if Dedf<1:\n",
    "            Dedf=1\n",
    "            #print('No, here is the trouble')\n",
    "            #return 0\n",
    "        \n",
    "        # compute G2 statistic\n",
    "        #print(S.shape)\n",
    "        #print(Sjk.shape)\n",
    "        #print(Sik.shape)\n",
    "        #print(Sk.shape)\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                for k in range(0,szCondVars):\n",
    "                    if (S[i][j][k]>0):\n",
    "                        #print(i,j,k)\n",
    "                        #test_op =  Sk[k]\n",
    "                        #test_op = Sik[i][k]\n",
    "                        #test_op = Sjk[j][k]\n",
    "                        G2=G2+(S[i][j][k] * np.log(\\\n",
    "                                    (S[i][j][k] * Sk[k])/\\\n",
    "                                    (Sik[i][k] * Sjk[j][k])))\n",
    "        G2 = 2*G2\n",
    "    \n",
    "    assoc = (alpha - (1 - chi2.cdf(G2,Dedf)))/alpha\n",
    "    #print(G2)\n",
    "    #print(Dedf)\n",
    "    #print(assoc)\n",
    "    if assoc<0:\n",
    "        assoc=0\n",
    "#     print(\"Target Node: \",TargetNode['name'])\n",
    "#     print(\"Node XI: \", Xi['name'])\n",
    "#     print(\"\\tG2:\",G2)\n",
    "#     print(\"\\tDedf: \",Dedf)\n",
    "#     print(\"\\tAssoc: \",assoc)\n",
    "#     print()\n",
    "#     print()\n",
    "    return assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getZsubset(bin_id,Z):\n",
    "    bin_str=str(bin_id[::-1])\n",
    "    Zsubset=[]\n",
    "    for i in range(0,len(bin_str)):\n",
    "        if bin_str[i]=='1':\n",
    "            Zsubset.append(Z[i])\n",
    "    return Zsubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrayUniverse(TargetNode,X):\n",
    "    Universe = []\n",
    "    for key in X:\n",
    "        #print(\"Key in X:\",key)\n",
    "        #print(\"Target Node is: \",TargetNode)\n",
    "        if (key!=TargetNode):\n",
    "            append_dict={'name':key,'data':X[key].copy(deep=True).tolist()}\n",
    "            Universe.append(append_dict)\n",
    "    return Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrayX(X):\n",
    "    returnable=[]\n",
    "    for key in X:\n",
    "        #tagged_c = MB_Column(X[key].copy(deep=True).tolist(),key)\n",
    "        append_dict={'name':key,'data':X[key].copy(deep=True).tolist()}\n",
    "        returnable.append(append_dict)\n",
    "    return returnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_names(dict_list):\n",
    "    print()\n",
    "    for item in dict_list:\n",
    "        print(item['name'],end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinAssoc(TargetNode, Xi,Z, fixedCondVars, X, alpha):\n",
    "    \n",
    "    min_assoc=999\n",
    "    #counter=0\n",
    "    #print(fixedCondVars)\n",
    "    if len(Z)==0:\n",
    "        #print(\"from minassoc1\")\n",
    "        #print(fixedCondVars)\n",
    "        min_assoc = Dep(TargetNode, Xi, fixedCondVars, X, alpha)\n",
    "        #counter+=1\n",
    "        subsetZ_min_assoc = fixedCondVars\n",
    "        #print(min_assoc)\n",
    "    else:\n",
    "        #print(2**len(Z)-1)\n",
    "        for IDsubsetZ_dec in range(0,2**len(Z)-1):\n",
    "            IDsubsetZ_bin = bin(IDsubsetZ_dec)\n",
    "            subsetZ = getZsubset(IDsubsetZ_bin,Z)\n",
    "            #print(len(Xi))\n",
    "            #print(\"from minassoc2\")\n",
    "            #print(IDsubsetZ_dec)\n",
    "            #print(subsetZ)\n",
    "            subsetZ_assoc=Dep(TargetNode, Xi, fixedCondVars+subsetZ,X,alpha)\n",
    "            #counter+=1\n",
    "            #print(subsetZ_assoc[IDsubsetZ_dec])\n",
    "            if subsetZ_assoc < min_assoc:\n",
    "                min_assoc = subsetZ_assoc\n",
    "                subsetZ_min_assoc = fixedCondVars + subsetZ\n",
    "                if (min_assoc==0):\n",
    "                    break\n",
    "    #print(\"Min Assoc module: \",counter)\n",
    "    return min_assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxMinHeuristic(TargetNode, CPC, Universe, X, alpha):\n",
    "    F=[]\n",
    "    assocF=-1\n",
    "    Z = CPC\n",
    "    fixedCondVars = []\n",
    "    if (len(CPC)>0):\n",
    "        Z = CPC[0:-1]           # all but the last one   \n",
    "        fixedCondVars = [CPC[-1]] # we use last one\n",
    "    \n",
    "    for i in range(len(Universe)-1,-1,-1):\n",
    "    #for i in range(0,len(Universe)):\n",
    "        if (len(Universe[i])==0):\n",
    "            continue\n",
    "        minAssoc = MinAssoc(TargetNode,Universe[i], Z,fixedCondVars, X,alpha)\n",
    "        #print(\"\\nFor \"+Universe[i]['name']+\"the minAssoc value was: \"+str(minAssoc))\n",
    "        if minAssoc <= 0.5:\n",
    "            #Universe[i]=[]\n",
    "            Universe.pop(i)\n",
    "            continue\n",
    "        if minAssoc>assocF:\n",
    "            assocF = minAssoc\n",
    "            F=Universe[i]\n",
    "        \n",
    "        #print(F)\n",
    "    return [F,assocF,Universe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_CPC(CPC):\n",
    "    for i in CPC:\n",
    "        print(i['name']+\" \", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMPC_(TargetNode,Universe,X,alpha):\n",
    "    \n",
    "    CPC=[]\n",
    "    #print(TargetNode)\n",
    "    # Phase I: Foward\n",
    "    print(\"Entering Phase I\")\n",
    "    print(\"MMPC_beggining: \\n\"+str(len(Universe)))\n",
    "    while len(Universe)>0:\n",
    "        CPC_old = list(CPC) # copy\n",
    "        maxminheur=MaxMinHeuristic(TargetNode,CPC,list(Universe),X,alpha)\n",
    "        # maxminheur = [F, assocF, Universe]\n",
    "        F = maxminheur[0]\n",
    "        assocF = maxminheur[1]\n",
    "        #print(assocF)\n",
    "        Universe = maxminheur[2]\n",
    "        #print(\"Universe printing line\")\n",
    "        #print(Universe)\n",
    "        #print(\"\\n=============================\")\n",
    "        #print(CPC)\n",
    "        if assocF > 0:\n",
    "            CPC.append(F)\n",
    "            indF=Universe.index(F)\n",
    "            Universe.pop(indF)\n",
    "        #if (len(CPC)==len(CPC_old)) or (len(CPC)>0.3*(len(Universe)-1)):\n",
    "        if (len(CPC)==len(CPC_old)):\n",
    "            break\n",
    "#         print(\"Universe actual size:\")\n",
    "#         print(len(Universe))\n",
    "#         print(\"CPC actual size:\")\n",
    "#         print(len(CPC))\n",
    "#         print(\"CPC contents:\")\n",
    "#         print_CPC(CPC)\n",
    "    \n",
    "    # Phase 2: Backward\n",
    "    print(\"\\nExiting MMPC_\")\n",
    "#     CPC=CPC[::-1]\n",
    "#     if len(CPC)>1:\n",
    "#         Z=list(CPC)\n",
    "#         for i in range(len(CPC)-1,-1,-1):\n",
    "#             # index is i\n",
    "#             Z.pop(i)\n",
    "#             #print(\"Analyzing D-separator for \",CPC[i]['name'])\n",
    "#             if ExistDseparator(TargetNode,CPC[i],Z,X,alpha)[0] == True:\n",
    "#                 #print(\"it did exist! removing from cpc.\")\n",
    "#                 CPC.pop(i)\n",
    "#     return symmetry_test_v2(TargetNode,CPC,X,alpha)\n",
    "    return CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMPC(TargetNode, X, alpha):\n",
    "    # The universe will be an array of DataFrame Columns\n",
    "    Universe = arrayUniverse(TargetNode,X)\n",
    "    print(len(Universe))\n",
    "    X = arrayX(X)\n",
    "    print(len(X))\n",
    "    for column in X:\n",
    "        if (column['name']==TargetNode):\n",
    "            TargetNode = column\n",
    "            break\n",
    "    CPC = MMPC_(TargetNode,Universe,X,alpha)\n",
    "    return CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMPC_Parallel(arg):\n",
    "    TargetNode, X, alpha = arg\n",
    "    Universe = arrayUniverse(TargetNode,X)\n",
    "    print(len(Universe))\n",
    "    X = arrayX(X)\n",
    "    print(len(X))\n",
    "    for column in X:\n",
    "        if (column['name']==TargetNode):\n",
    "            TargetNode = column\n",
    "            break\n",
    "    CPC = MMPC_(TargetNode,Universe,X,alpha)\n",
    "    return CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeysPartition(targetNode, data_keys,n_partitions):\n",
    "    n_partitions = int(np.log2(len(data_keys)))*2*2  # log2 ( number of variables ) divided by 2\n",
    "    partitions = []\n",
    "    partition_diff = int(int(len(data_keys)/n_partitions)/2)\n",
    "    for i in range(0,n_partitions):\n",
    "        if (i==0):\n",
    "            partitions.append(list(data_keys[-partition_diff::])+list(data_keys[0:int(len(data_keys)/n_partitions)]))\n",
    "            #partitions.append(list(data_keys[0:int(len(data_keys)/n_partitions)]))\n",
    "            continue\n",
    "        if (i<n_partitions-1):\n",
    "            partitions.append(list(data_keys[-partition_diff+int(len(data_keys)/n_partitions)*i:int(len(data_keys)/n_partitions)*(i+1)]))\n",
    "            #partitions.append(list(data_keys[int(len(data_keys)/n_partitions)*i:int(len(data_keys)/n_partitions)*(i+1)]))\n",
    "            continue\n",
    "        if (i==n_partitions-1):\n",
    "            partitions.append(list(data_keys[-partition_diff+int(len(data_keys)/n_partitions)*i::]))\n",
    "            #partitions.append(list(data_keys[int(len(data_keys)/n_partitions)*i::]))\n",
    "            \n",
    "    for i in range(0,n_partitions):\n",
    "        for j in range(len(partitions[i])-1,-1,-1):\n",
    "            if (partitions[i][j]==targetNode):\n",
    "                partitions[i].pop(j)\n",
    "        partitions[i] = partitions[i] + [targetNode]\n",
    "    return partitions,n_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alpha=0.05\n",
    "print(\"======================================================================================\")\n",
    "print(\"Init Genetic Algorithm\")\n",
    "print(\"======================================================================================\")\n",
    "main_directory= '/home/a20114261/GeneInteractions/GeneInteractionsBN_Datasets/Balanced_Batches/censored_Erk/'\n",
    "if not os.path.exists(main_directory):\n",
    "    print(\"Bad routing.\")\n",
    "PCs=[]\n",
    "for filename in os.listdir(main_directory)[:1]:\n",
    "    print(\"Analizing \"+filename)\n",
    "    X = pd.read_csv(main_directory+'/'+filename)\n",
    "    #print(X.keys())\n",
    "    # each cell can be -1, 0 or 1\n",
    "    #Pre-processing data\n",
    "    #X['Pvalue']=list(map(lambda x: 1 if x<=0.01 else (0 if x>=0.5 else -1), X['Pvalue']))\n",
    "    X['Pvalue']=list(map(lambda x: 1 if x<=0.01 else 0, X['Pvalue']))\n",
    "    #del X['Unnamed: 0']\n",
    "    #del X['TrueIndex']\n",
    "    del X['CauseGene']\n",
    "    del X['EffectGene']\n",
    "    del X['Replicate']\n",
    "    del X['Treatment']\n",
    "    PC=[]\n",
    "    for column in X:\n",
    "        if (column != \"Pvalue\"):\n",
    "            X[column] = list(map(lambda x: x+1 , X[column]))\n",
    "    x_heads = []\n",
    "    for i in range(0,len(X.keys())):\n",
    "        max_v=1\n",
    "        for row in X[X.keys()[i]]:\n",
    "            if (row>max_v-1):\n",
    "                max_v+=row-(max_v-1)\n",
    "        save_i=i+1\n",
    "        x_heads.append(\"Node\"+str(save_i)+\"@\"+str(max_v))\n",
    "    X.columns = x_heads\n",
    "    data_c_size = len(X[X.keys()[1]])\n",
    "    data_vars = [0,1,2,3]\n",
    "    print(\"======================================================================================\")\n",
    "    TargetNode =X.keys()[0]\n",
    "    n_partitions = 0\n",
    "    keys_partitions, n_partitions = getKeysPartition(TargetNode,X.keys(),n_partitions)\n",
    "    arg_instances = []\n",
    "    for i in range(0,n_partitions):\n",
    "        arg_instances.append((TargetNode,X[keys_partitions[i]],alpha))\n",
    "    PC=Parallel(n_jobs=n_partitions)(map(delayed(MMPC_Parallel),arg_instances))\n",
    "    PC_dict={}\n",
    "    PC_dict['pcs']=PC\n",
    "    PC_dict['filename']=filename\n",
    "    PCs.append(PC_dict)\n",
    "# for after\n",
    "n_parallel_forks = n_partitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_universe=[]\n",
    "for PC_dict in PCs:\n",
    "    PC_train={}\n",
    "    PC_train['dataset']=PC_dict['filename']\n",
    "    PC_train['supercpc']=[]\n",
    "    for pc_subset in PC_dict['pcs']:\n",
    "        if (len(pc_subset)==0):\n",
    "            continue\n",
    "        for e in pc_subset:\n",
    "            PC_train['supercpc'].append(e['name'])\n",
    "    PC_train['supercpc']=list(set(PC_train['supercpc']))\n",
    "    initial_universe.append(PC_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "PCs_mean_scores=[]\n",
    "for i in range(0,1000,100):\n",
    "    if (i==0):\n",
    "        continue\n",
    "    PC_mean_score=[]\n",
    "    for PC_set in initial_universe:    \n",
    "        start = time.time()\n",
    "        print(\"For \"+PC_set['dataset'])\n",
    "        print(\"CPC of length \"+str(len(PC_set['supercpc'])))\n",
    "        PC_mean_score.append(CandidateScore(PC_set,i)['score'])    \n",
    "        end = time.time()\n",
    "        print(\"Time elapsed: \"+str(end - start)+\" seconds\")\n",
    "        print(\"=================================================================\")\n",
    "    print(\"Mean score for Intercept == \"+str(i)+\" was: \"+str(sum(PC_mean_score)/len(PC_mean_score)))\n",
    "    print(\"/////////////////////////////////////////////////////////////////////\")\n",
    "    PCs_mean_scores.append(sum(PC_mean_score)/len(PC_mean_score))\n",
    "    print(\"/////////////////////////////////////////////////////////////////////\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeCopyPcs = list(PCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "poblation_gen_init = []\n",
    "Y_pred = []\n",
    "Y_pred = X_test[X_test.keys()[0]]\n",
    "\n",
    "for PC_dict in PCs:\n",
    "    for pc_subset in PC_dict['pcs']:\n",
    "        # Structure for poblation in genetic algorithm will be like this\n",
    "        # ->cpc\n",
    "        # ->score\n",
    "        PobDict={}\n",
    "        PobDict['cpc']=[]\n",
    "        for e in pc_subset:\n",
    "        PobDict['filenames']=[]\n",
    "        PobDict['filenames'].append(filename)\n",
    "        #PobDict['score']=clf.score(X_pred,Y_pred)\n",
    "        poblation_gen_init.append(PobDict)\n",
    "        #print(PobDict['score'])\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parallel processing\n",
    "scored_init_pob = Parallel(n_jobs=n_parallel_forks)(map(delayed(CandidateScore),poblation_gen_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CandidateScore(PobDict,intercept_scaling):\n",
    "    global X_test\n",
    "    global x_heads\n",
    "\n",
    "    # training and scoring\n",
    "    X_train_df = pd.read_csv(main_directory+PobDict['dataset'])\n",
    "    del X_train_df['CauseGene']\n",
    "    del X_train_df['EffectGene']\n",
    "    del X_train_df['Replicate']\n",
    "    del X_train_df['Treatment']\n",
    "    X_train_df['Pvalue']=list(map(lambda x: 1 if x<=0.01 else 0, X_train_df['Pvalue']))\n",
    "    x_heads[0]='Node1'\n",
    "    X_train_df.columns=x_heads\n",
    "    X_test.columns=x_heads\n",
    "    #print(X_train_df.keys())\n",
    "    Y_train=X_train_df[\"Node1\"]\n",
    "    \n",
    "    \n",
    "    # training classifier\n",
    "    clf = LinearSVC(loss='hinge', C=0.001, intercept_scaling=intercept_scaling)\n",
    "    clf.fit(X_train_df[PobDict['supercpc']],Y_train)\n",
    "    \n",
    "    # get predicted values for TargetNode\n",
    "    #Y_pred=clf.predict(X_test[PobDict['supercpc'][:10]])\n",
    "    \n",
    "    # get score values\n",
    "    precision = []\n",
    "    Y_pred_0 = clf.predict(X_test.query('Node1 ==0')[PobDict['supercpc']])\n",
    "    precision.append(Y_pred_0.tolist().count(0)/len(Y_pred_0))\n",
    "    Y_pred_1 = clf.predict(X_test.query('Node1 ==1')[PobDict['supercpc']])\n",
    "    precision.append(Y_pred_1.tolist().count(1)/len(Y_pred_1))\n",
    "    \n",
    "    #precision.append(clf.score(X_test.query('Node1 ==0')[PobDict['supercpc'][:10]],X_test.query('Node1 ==0')[['Node1']]))\n",
    "    #precision.append(clf.score(X_test.query('Node1 ==1')[PobDict['supercpc'][:10]],X_test.query('Node1 ==1')[['Node1']]))\n",
    "    print('precision for class 0: '+str(precision[0]))\n",
    "    print('precision for class 1: '+str(precision[1]))\n",
    "    \n",
    "    print(\"G-mean score: \"+str(g_mean(precision[0],precision[1])))\n",
    "    #print(clf.score(X_test[PobDict['supercpc'][:10]],Y_test))\n",
    "    PobDict['score']=g_mean(precision[0],precision[1])\n",
    "    return PobDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeCopyScoredPoblation = list(scored_init_pob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the poblation\n",
    "from operator import itemgetter\n",
    "poblation_sorted = sorted(scored_init_pob, key=itemgetter('score'), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging results\n",
    "# unfinished\n",
    "counter_p = 1\n",
    "for cpc in poblation_sorted:    \n",
    "    print(\"===========================================\")\n",
    "    print(\"CPC n\"+str(counter_p)+\" with score: \"+str(cpc['score']))\n",
    "    counter_p=counter_p+1\n",
    "    print(cpc['cpc'])\n",
    "    print(\"===========================================\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Node_inside(cpc, cpc_node):\n",
    "    for e in cpc:\n",
    "        if (e==cpc_node):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_parent_pob = list(train_test_split(poblation_sorted,train_size=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cross_parent_pob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=list(range(0,10,2))\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "test_list_1=[[1,2,3,4],[5,6,7,8]]\n",
    "\n",
    "\n",
    "test_list_1 = list(itertools.chain.from_iterable(test_list_1))\n",
    "test_list_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poblation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# crossover\n",
    "# we will take the alpha % of the best poblation to do the crossover\n",
    "alpha_crossover = 0.50\n",
    "\n",
    "# we will make beta childs per couple\n",
    "beta_crossover = 1\n",
    "\n",
    "# probability of taking an atribute present in both groups\n",
    "thetha_crossover = 0.95\n",
    "\n",
    "# probability of taking an atribute not present in both groups\n",
    "sigma_crossover = 0.40\n",
    "\n",
    "# poblation selected\n",
    "crossover_parent_poblation = list(train_test_split(poblation_sorted,train_size=alpha_crossover))[0]\n",
    "\n",
    "# crossover\n",
    "crossover_child_poblation=[]\n",
    "\n",
    "crossover_parallel_results=[]\n",
    "\n",
    "step_parents = 4\n",
    "\n",
    "for cpc_a_counter in range(0,len(crossover_parent_poblation),step_parents):\n",
    "    if (cpc_a_counter+step_parents>=len(crossover_parent_poblation)):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    cpc_a = {}\n",
    "    cpc_a['cpc']=[]\n",
    "    cpc_a['filenames']=[]\n",
    "    # appending the fathers from a\n",
    "    \n",
    "        \n",
    "    for cpc_group_a_counter in range(cpc_a_counter,cpc_a_counter+step_parents):\n",
    "        for cpc_name in crossover_parent_poblation[cpc_group_a_counter]['cpc']:\n",
    "            cpc_a['cpc'].append(cpc_name)\n",
    "        for cpc_filename in crossover_parent_poblation[cpc_group_a_counter]['filenames']:\n",
    "            cpc_a['filenames'].append(cpc_filename)\n",
    "    \n",
    "    for cpc_b_counter in range(0,len(crossover_parent_poblation)):\n",
    "        if (cpc_b_counter+step_parents>=len(crossover_parent_poblation)):\n",
    "            break\n",
    "        \n",
    "        \n",
    "        cpc_b={}\n",
    "        cpc_b['cpc']=[]\n",
    "        cpc_b['filenames']=[]\n",
    "        #appending fathers from b\n",
    "        for cpc_group_b_counter in range(cpc_b_counter,cpc_b_counter+step_parents):\n",
    "            for cpc_name in crossover_parent_poblation[cpc_group_b_counter]['cpc']:\n",
    "                cpc_b['cpc'].append(cpc_name)\n",
    "            for cpc_filename in crossover_parent_poblation[cpc_group_b_counter]['filenames']:\n",
    "                cpc_b['filenames'].append(cpc_filename)\n",
    "    \n",
    "        if (cpc_a_counter==cpc_b_counter):\n",
    "            continue\n",
    "        for crossover_counter in range(0,beta_crossover):\n",
    "            # first, we select the atributes present in both cpcs\n",
    "            PobDict={}\n",
    "            \n",
    "            cross_child=[]\n",
    "            for node_a in cpc_a['cpc']:\n",
    "                for node_b in cpc_b['cpc']:\n",
    "                    if (node_a==node_b):\n",
    "                        # diversity conditioning\n",
    "                        prob_res = random.random()\n",
    "                        if (prob_res<thetha_crossover):\n",
    "                            cross_child.append(node_a)\n",
    "                            continue\n",
    "            # for parent a\n",
    "            for node_a in cpc_a['cpc']:\n",
    "                if (Node_inside(cross_child,node_a)==False):\n",
    "                    prob_res = random.random()\n",
    "                    if (prob_res<sigma_crossover):\n",
    "                        cross_child.append(node_a)\n",
    "            # for parent b\n",
    "            for node_b in cpc_b['cpc']:\n",
    "                if (Node_inside(cross_child,node_b)==False):\n",
    "                    prob_res = random.random()\n",
    "                    if (prob_res<sigma_crossover):\n",
    "                        cross_child.append(node_b)\n",
    "            \n",
    "            if (len(cross_child)==0):\n",
    "                continue\n",
    "            PobDict['cpc']=sorted(cross_child)\n",
    "            \n",
    "            \n",
    "            # filenames\n",
    "            PobDict['filenames']=[]\n",
    "            for filename in cpc_a['filenames']:\n",
    "                PobDict['filenames'].append(filename)\n",
    "            for filename in cpc_b['filenames']:\n",
    "                try:\n",
    "                    try_var=PobDict['filenames'].index(filename)\n",
    "                except:\n",
    "                    PobDict['filenames'].append(filename)\n",
    "            crossover_child_poblation.append(PobDict)\n",
    "scored_child_pob = Parallel(n_jobs=n_parallel_forks)(map(delayed(CandidateScore),crossover_child_poblation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_poblation_sorted = sorted(scored_child_pob, key=itemgetter('score'), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "180*0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(child_poblation_sorted[:int(poblation_size*(1-new_pob_perc))-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "new_pob_perc=0.40\n",
    "# ideally, we will use 180 as cap for the poblation\n",
    "poblation_size=180\n",
    "for genetic_algorithm_counter in range(0,100):\n",
    "    new_pob = []\n",
    "    for e in child_poblation_sorted[:int(poblation_size*(1-new_pob_perc))-1]:\n",
    "        new_pob.append(e)\n",
    "    for e in list(shuffle(poblation_sorted))[:int(poblation_size*new_pob_perc)-1]:\n",
    "        new_pob.append(e)\n",
    "    # poblation selected\n",
    "    crossover_parent_poblation = list(train_test_split(new_pob,train_size=alpha_crossover))[0]\n",
    "\n",
    "    # crossover\n",
    "    crossover_child_poblation=[]\n",
    "\n",
    "    crossover_parallel_results=[]\n",
    "\n",
    "    for cpc_a_counter in range(0,len(crossover_parent_poblation)):\n",
    "        cpc_a = crossover_parent_poblation[cpc_a_counter]\n",
    "        for cpc_b_counter in range(0,len(crossover_parent_poblation)):\n",
    "            cpc_b = crossover_parent_poblation[cpc_b_counter]\n",
    "            if (cpc_a_counter==cpc_b_counter):\n",
    "                continue\n",
    "            for crossover_counter in range(0,beta_crossover):\n",
    "                # first, we select the atributes present in both cpcs\n",
    "                PobDict={}\n",
    "\n",
    "                cross_child=[]\n",
    "                for node_a in cpc_a['cpc']:\n",
    "                    for node_b in cpc_b['cpc']:\n",
    "                        if (node_a==node_b):\n",
    "                            # diversity conditioning\n",
    "                            prob_res = random.random()\n",
    "                            if (prob_res<thetha_crossover):\n",
    "                                cross_child.append(node_a)\n",
    "                                continue\n",
    "                # for parent a\n",
    "                for node_a in cpc_a['cpc']:\n",
    "                    if (Node_inside(cross_child,node_a)==False):\n",
    "                        prob_res = random.random()\n",
    "                        if (prob_res<sigma_crossover):\n",
    "                            cross_child.append(node_a)\n",
    "                # for parent b\n",
    "                for node_b in cpc_b['cpc']:\n",
    "                    if (Node_inside(cross_child,node_b)==False):\n",
    "                        prob_res = random.random()\n",
    "                        if (prob_res<sigma_crossover):\n",
    "                            cross_child.append(node_b)\n",
    "\n",
    "                if (len(cross_child)==0):\n",
    "                    continue\n",
    "                PobDict['cpc']=sorted(cross_child)\n",
    "\n",
    "\n",
    "                # filenames\n",
    "                PobDict['filenames']=[]\n",
    "                for filename in cpc_a['filenames']:\n",
    "                    PobDict['filenames'].append(filename)\n",
    "                for filename in cpc_b['filenames']:\n",
    "                    try:\n",
    "                        try_var=PobDict['filenames'].index(filename)\n",
    "                    except:\n",
    "                        PobDict['filenames'].append(filename)\n",
    "                crossover_child_poblation.append(PobDict)\n",
    "    scored_child_pob = Parallel(n_jobs=n_parallel_forks*2)(map(delayed(CandidateScore),crossover_child_poblation))\n",
    "    child_poblation_sorted = sorted(scored_child_pob+new_pob, key=itemgetter('score'), reverse=True)\n",
    "    print(\"Round n \"+str(genetic_algorithm_counter))\n",
    "    print(child_poblation_sorted[0]['filenames'])\n",
    "    print(child_poblation_sorted[0]['cpc'])\n",
    "    print(child_poblation_sorted[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in child_poblation_sorted:\n",
    "    print(\"==============================================================================\")\n",
    "    print(e['filenames'])\n",
    "    print(e['cpc'])\n",
    "    print(e['score'])\n",
    "    print(\"==============================================================================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poblation_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alpha=0.05\n",
    "# Initial vectors setup\n",
    "#const_vectors = setup_const_vectors(data_vars,data_c_size)\n",
    "#for i in range(0,len(X.keys())):    \n",
    "#for i in range(0,1):  \n",
    "\n",
    "print(\"======================================================================================\")\n",
    "TargetNode =X.keys()[0]\n",
    "n_partitions = 0\n",
    "keys_partitions, n_partitions = getKeysPartition(TargetNode,X.keys(),n_partitions)\n",
    "arg_instances = []\n",
    "for i in range(0,n_partitions):\n",
    "    arg_instances.append((TargetNode,X[keys_partitions[i]],alpha))\n",
    "# we know that core number is 8\n",
    "n_cores=-1\n",
    "if (n_partitions>8):\n",
    "    n_cores = 8\n",
    "else:\n",
    "    n_cores=n_partitions\n",
    "PC=(map(MMPC_Parallel,arg_instances))\n",
    "\n",
    "total_PC = []\n",
    "for pc_element in PC:\n",
    "    total_PC=total_PC+pc_element\n",
    "TargetNodeDict={}\n",
    "TargetNodeDict['name'] = TargetNode\n",
    "TargetNodeDict['data'] = X[TargetNode]\n",
    "print(\"Target Node was: \"+TargetNodeDict['name'])\n",
    "for i in range(len(total_PC)-1,-1,-1):\n",
    "    for j in range(len(total_PC)-1,-1,-1):\n",
    "        if i != j:\n",
    "            if total_PC[i]['name']==total_PC[j]['name']:\n",
    "                total_PC.pop(i)\n",
    "                break\n",
    "print(\"PC set is :\")\n",
    "for j in total_PC:\n",
    "    print(j['name'])\n",
    "    \n",
    "clf = LinearSVC(random_state=0)\n",
    "X_val=[]\n",
    "Y=[]\n",
    "X_arr=arrayX(X)\n",
    "for i in range(0,data_c_size):\n",
    "    d_row = []\n",
    "    for pc_column in X_arr:\n",
    "        d_row.append(pc_column['data'][i])\n",
    "    X_val.append(d_row)\n",
    "for i in range(0,data_c_size):\n",
    "    Y.append(TargetNodeDict['data'][i])\n",
    "clf.fit(X_val[:-1000],Y[:-1000])\n",
    "print(clf.score(X_val[-1000:],Y[-1000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase 2: Backward\n",
    "def symmetry_test(TargetNode,CPC,X,alpha):\n",
    "    print(\"\\nEntering Phase II\")\n",
    "    if len(CPC)>1:\n",
    "        Z=list(CPC)\n",
    "        for i in range(len(CPC)-1,-1,-1):\n",
    "            # index is i\n",
    "            Z.pop(i)\n",
    "            print(\"Analyzing D-separator for \",CPC[i]['name'])\n",
    "            if ExistDseparator(TargetNode,CPC[i],Z,X,alpha)[0] == True:\n",
    "                print(\"it did exist! removing from cpc.\")\n",
    "                CPC.pop(i)\n",
    "    return CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified symmetry  test\n",
    "def symmetry_test_v2(TargetNode, CPC, X,alpha):\n",
    "    print(\"\\nEntering Phase II\")\n",
    "    if (len(CPC)>1):\n",
    "        Z=list(CPC)\n",
    "        while len(Z)>0:\n",
    "            # Will test all vs the Target, and select the lowest\n",
    "            minDep=999\n",
    "            minDepIndex=-1\n",
    "            for i in range(0,len(Z)):\n",
    "                zCopy = list(Z)\n",
    "                zCopy.pop(i)\n",
    "                auxDep = Dep(TargetNode,Z[i],zCopy,X,alpha)\n",
    "                if (minDep>auxDep):\n",
    "                    minDep = auxDep\n",
    "                    minDepIndex=i\n",
    "            # Find index in CPC\n",
    "            CPCindex=-1\n",
    "            for i in range(0,len(CPC)):\n",
    "                if (CPC[i]['name']==Z[minDepIndex]['name']):\n",
    "                    CPCindex=i\n",
    "                    break\n",
    "            \n",
    "            # Remove from Z\n",
    "            Z.pop(minDepIndex)\n",
    "            print_names(Z)\n",
    "            print(\"Analyzing D-separator for \",CPC[CPCindex]['name'])\n",
    "            if ExistDseparator(TargetNode,CPC[CPCindex],Z,X,alpha)[0] == True:\n",
    "                print(\"it did exist! removing from cpc.\")\n",
    "                CPC.pop(CPCindex)\n",
    "    return CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_dep_parallel(TargetNode, CPC, X,alpha):    \n",
    "    print(\"\\nEntering Phase II\")\n",
    "    if (len(CPC)>1):\n",
    "        Z=list(CPC)\n",
    "        while len(Z)>0:\n",
    "            # Will test all vs the Target, and select the lowest\n",
    "            minDep=999\n",
    "            minDepIndex=-1\n",
    "            arg_instances = []\n",
    "            for i in range(0,len(Z)):\n",
    "                zCopy = list(Z)\n",
    "                zCopy.pop(i)\n",
    "                arg_instances.append((TargetNode,Z[i],zCopy,X,alpha))\n",
    "                #auxDep = Dep(TargetNode,Z[i],zCopy,X,alpha)\n",
    "            depResults = Parallel(n_jobs=8)(map(delayed(Dep_Parallel),arg_instances))\n",
    "            for i in range(0,len(depResults)):\n",
    "                if (minDep>depResults[i]):\n",
    "                    minDep = depResults[i]\n",
    "                    minDepIndex=i\n",
    "            # Find index in CPC\n",
    "            CPCindex=-1\n",
    "            for i in range(0,len(CPC)):\n",
    "                if (CPC[i]['name']==Z[minDepIndex]['name']):\n",
    "                    CPCindex=i\n",
    "                    break\n",
    "            \n",
    "            # Remove from Z\n",
    "            Z.pop(minDepIndex)\n",
    "            print_names(Z)\n",
    "            print(\"Analyzing D-separator for \",CPC[CPCindex]['name'])\n",
    "            if ExistDseparator(TargetNode,CPC[CPCindex],Z,X,alpha)[0] == True:\n",
    "                print(\"it did exist! removing from cpc.\")\n",
    "                CPC.pop(CPCindex)\n",
    "    return CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dep_Parallel(args):\n",
    "    #print('in Dep')\n",
    "    TargetNode, Xi, CondVars, X, alpha = args\n",
    "    global data_c_size\n",
    "    \n",
    "    vvTarget = []\n",
    "    for i in range(int(TargetNode['name'].split('@')[1])):\n",
    "        vvTarget.append(i)\n",
    "    \n",
    "    vvXi = []\n",
    "    for i in range(int(Xi['name'].split('@')[1])):\n",
    "        vvXi.append(i)\n",
    "    \n",
    "    szCondVars = 1\n",
    "    #print(CondVars)\n",
    "    for column in CondVars:\n",
    "        #print(column)\n",
    "        szCondVars *= int(column['name'].split('@')[1])\n",
    "    \n",
    "    if data_c_size <= (5 * len(vvTarget)*len(vvXi)*(szCondVars)):\n",
    "        return 1\n",
    "    \n",
    "    if (len(CondVars)==0):\n",
    "        S = np.zeros((len(vvTarget),len(vvXi)))\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                var1=np.array(TargetNode['data']) # Target Node\n",
    "                var2=np.array(Xi['data'])\n",
    "                # looking up for 0, 1 and 2\n",
    "                #op1 = tf_mask_var(var1,vvTarget[i])\n",
    "                op1 = np.array(list(map(lambda x: 1 if x==vvTarget[i] else 0, var1.tolist())))\n",
    "                #op2 = tf_mask_var(var2,vvXi[j])\n",
    "                op2 = np.array(list(map(lambda x: 1 if x==vvXi[j] else 0, var2.tolist())))\n",
    "                #S[i][j] = tf_dot_mult(op1,op2)\n",
    "                S[i][j]=np.sum(op1*op2)\n",
    "                #print(S[i][j])\n",
    "        G2=0\n",
    "        N = np.sum(S)\n",
    "#         print(S)\n",
    "#         print(N)\n",
    "        Si =np.sum(S,axis=1)\n",
    "        Sj =np.sum(S,axis=0)\n",
    "        Df = ((len(vvTarget)-1)*(len(vvXi)-1))\n",
    "#         print(S[np.where(S>0)])\n",
    "        Dedf = len((S[np.where(S>0)]))\\\n",
    "                      - len(Si[np.where(Si>0)])\\\n",
    "                      - len(Sj[np.where(Sj>0)]) +1\n",
    "#         print(\"Dedf before if: \",Dedf)\n",
    "        if (Dedf<1):\n",
    "            Dedf=1\n",
    "            #print('Here is the trouble')\n",
    "            #return 0\n",
    "        #print(S)\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                #print(i)\n",
    "                #print(j)\n",
    "                if (S[i][j]>0):\n",
    "                    #print(Si[i])\n",
    "                    #print(Sj[j])\n",
    "                    G2 = G2 + S[i][j]*np.log((S[i][j])*N/(Si[i]*Sj[j]))\n",
    "                    #print(G2)\n",
    "        G2 = 2*G2\n",
    "        \n",
    "    else: # test conditional dependency\n",
    "        \n",
    "            \n",
    "        S = np.zeros((len(vvTarget),len(vvXi),szCondVars))\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                for k in range(0,szCondVars):\n",
    "                    condValue = GetCondValues(k,CondVars)\n",
    "                    #print(condValue)\n",
    "                    flagDataCondVars = np.ones(len(Xi['data']))\n",
    "                    for l in range(0,len(CondVars)):\n",
    "                        X_l = CondVars[l]\n",
    "                        flagDataCondVars = flagDataCondVars*np.array(list(map(lambda x: 1 if x==condValue[l] else 0, X_l['data'])))\n",
    "                        #op1 = tf_mask_var(X_l['data'],condValue[l])\n",
    "                        #flagDataCondVars = np.array(list(map(lambda x: 1 if x==condValue[l] else 0, X_l.tolist())))\n",
    "                        #flagDataCondVars = flagDataCondVars*op1\n",
    "                    var1=np.array(TargetNode['data'])\n",
    "                    var2=np.array(Xi['data'])\n",
    "                    # looking up for -1, 0 and 1\n",
    "                    op1 = np.array(list(map(lambda x: 1 if x==vvTarget[i] else 0, var1.tolist())))\n",
    "                    #op1 = tf_mask_var(var1,vvTarget[i])\n",
    "                    #print(op1)\n",
    "                    #op2 = tf_mask_var(var2,vvXi[j])\n",
    "                    op2 = np.array(list(map(lambda x: 1 if x==vvXi[j] else 0, var2.tolist())))\n",
    "                    #print(op2)\n",
    "                    #print(op1*op2*flagDataCondVars)\n",
    "                    S[i][j][k]=np.sum(op1*op2*flagDataCondVars)\n",
    "        G2 = 0\n",
    "        \n",
    "        Sjk = np.sum(S,axis=0)\n",
    "        Sik = np.sum(S,axis=1)\n",
    "        Sk = np.sum(Sjk,axis=0)\n",
    "        \n",
    "        #Sjk = sum(S,1);\n",
    "        #Sik = sum(S,2);\n",
    "        #Sk = sum(Sjk,2); \n",
    "        Dedf = len(S[np.where(S>0)]) -\\\n",
    "                len(Sik[np.where(Sik>0)]) -\\\n",
    "                len(Sjk[np.where(Sjk>0)]) +\\\n",
    "                len(Sk[np.where(Sk>0)])\n",
    "        if Dedf<1:\n",
    "            Dedf=1\n",
    "            #print('No, here is the trouble')\n",
    "            #return 0\n",
    "        \n",
    "        # compute G2 statistic\n",
    "        #print(S.shape)\n",
    "        #print(Sjk.shape)\n",
    "        #print(Sik.shape)\n",
    "        #print(Sk.shape)\n",
    "        for i in range(0,len(vvTarget)):\n",
    "            for j in range(0,len(vvXi)):\n",
    "                for k in range(0,szCondVars):\n",
    "                    if (S[i][j][k]>0):\n",
    "                        #print(i,j,k)\n",
    "                        #test_op =  Sk[k]\n",
    "                        #test_op = Sik[i][k]\n",
    "                        #test_op = Sjk[j][k]\n",
    "                        G2=G2+(S[i][j][k] * np.log(\\\n",
    "                                    (S[i][j][k] * Sk[k])/\\\n",
    "                                    (Sik[i][k] * Sjk[j][k])))\n",
    "        G2 = 2*G2\n",
    "    \n",
    "    assoc = (alpha - (1 - chi2.cdf(G2,Dedf)))/alpha\n",
    "    #print(G2)\n",
    "    #print(Dedf)\n",
    "    #print(assoc)\n",
    "    if assoc<0:\n",
    "        assoc=0\n",
    "#     print(\"Target Node: \",TargetNode['name'])\n",
    "#     print(\"Node XI: \", Xi['name'])\n",
    "#     print(\"\\tG2:\",G2)\n",
    "#     print(\"\\tDedf: \",Dedf)\n",
    "#     print(\"\\tAssoc: \",assoc)\n",
    "#     print()\n",
    "#     print()\n",
    "    return assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0)\n",
    "X_val=[]\n",
    "Y=[]\n",
    "X=arrayX(X)\n",
    "for i in range(0,data_c_size):\n",
    "    d_row = []\n",
    "    for pc_column in X:\n",
    "        d_row.append(pc_column['data'][i])\n",
    "    X_val.append(d_row)\n",
    "for i in range(0,data_c_size):\n",
    "    Y.append(TargetNodeDict['data'][i])\n",
    "clf.fit(X_val[:-1000],Y[:-1000])\n",
    "print(clf.score(X_val[-1000:],Y[-1000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_arr = ['hello','there']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
