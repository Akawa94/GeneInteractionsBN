{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.stats import chi2\n",
    "from math import floor\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.svm import LinearSVC\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support as full_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading preselected nodes\n",
    "main_directory= '/home/a20114261/alarm_datasets/Alarm10/'\n",
    "save_folder='/home/a20114261/alarm_datasets/log_results/'\n",
    "\n",
    "if not os.path.exists(main_directory):\n",
    "    print(\"Bad routing.\")\n",
    "preselected_nodes=[] # will have [node_str,balance_ranking]\n",
    "# so far, we need Node, size of estimated pc set per filename, balance ranking\n",
    "ff = open(save_folder+'mmpc_partitioned/alarm10_binary_nodes_selected.txt','r')\n",
    "for e in ff.read().split('\\n'):\n",
    "    spl_line = e.split(';')\n",
    "    if (len(spl_line[0])==0):\n",
    "        continue\n",
    "    preselected_nodes.append([spl_line[0],float(spl_line[1])])\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading markov blanket for each preselected node\n",
    "X_graph = pd.read_csv('/home/a20114261/Alarm10_graph.txt',delimiter='  ',header=None)\n",
    "graph_heads=[]\n",
    "for i in range(1,len(X_graph.keys())+1):\n",
    "    graph_heads.append('Node'+str(i))\n",
    "X_graph.columns = graph_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def related_nodes(X_graph,graph_heads,ind):\n",
    "    print(\"Acoording to the true graph, the PC set for \",X_graph.keys()[ind],\"is:\")\n",
    "    related = []\n",
    "    \n",
    "    for i in range(0,len(X_graph[X_graph.keys()[ind]])):\n",
    "        if (X_graph[X_graph.keys()[ind]][i]==1):\n",
    "            print(X_graph.keys()[i])\n",
    "            related.append(X_graph.keys()[i])\n",
    "    X_graph = X_graph.transpose()\n",
    "    X_graph.columns = graph_heads\n",
    "    for i in range(0,len(X_graph[X_graph.keys()[ind]])):\n",
    "        if (X_graph[X_graph.keys()[ind]][i]==1):\n",
    "            print(X_graph.keys()[i])\n",
    "            related.append(X_graph.keys()[i])\n",
    "    X_graph = X_graph.transpose()\n",
    "    X_graph.columns = graph_heads\n",
    "    \n",
    "    return related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_related = []\n",
    "for i in range(0,len(X_graph.keys())):    \n",
    "    print(\"====================================================\")\n",
    "    testing_related.append(related_nodes(X_graph,graph_heads,i))\n",
    "\n",
    "print(testing_related)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(X_graph.keys())):    \n",
    "    print(\"====================================================\")\n",
    "    related_nodes(X_graph,graph_heads,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alpha=0.05\n",
    "print(\"======================================================================================\")\n",
    "print(\"True MB Evaluation\")\n",
    "print(\"======================================================================================\")\n",
    "\n",
    "\n",
    "PCs=[]\n",
    "for filename in os.listdir(main_directory):\n",
    "    print(\"Linking \"+filename+\" to the Markov Blanket of each preselected node\")\n",
    "#     X = pd.read_csv(main_directory+'/'+filename,delimiter='  ',header=None)\n",
    "#     X.columns = graph_heads\n",
    "    print(\"======================================================================================\")\n",
    "    for TargetNodeSelected in [x[0].split('@')[0] for x in preselected_nodes]:\n",
    "    #  -----------------    \n",
    "        PC_dict={}\n",
    "        PC_dict['TargetNode']=TargetNodeSelected\n",
    "        PC_dict['PC']=related_nodes(X_graph,graph_heads,int(TargetNodeSelected.split('Node')[1])-1)\n",
    "        PC_dict['filename']=filename\n",
    "        PCs.append(PC_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(PCs)-1,-1,-1):\n",
    "    if (len(PCs[i]['PC'])==0):\n",
    "        PCs.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# evaluation for each node, using each file as training set, in parallel\n",
    "print(\"======================================================================================\")\n",
    "print(\"True-MB Scoring phase\")\n",
    "print(\"======================================================================================\")\n",
    "\n",
    "PC_mean_score = Parallel(n_jobs=40)(map(delayed(CandidateScore_Parallel),PCs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PC': ['Node23', 'Node76', 'Node96', 'Node128'],\n",
       " 'TargetNode': 'Node101',\n",
       " 'class_precisions': [[[[0, 26, 0.0], [1, 974, 1.0]], 'Alarm10_s1000_v1.txt'],\n",
       "  [[[0, 15, 0.0], [1, 985, 1.0]], 'Alarm10_s1000_v6.txt'],\n",
       "  [[[0, 22, 0.0], [1, 978, 1.0]], 'Alarm10_s1000_v5.txt'],\n",
       "  [[[0, 23, 0.0], [1, 977, 1.0]], 'Alarm10_s1000_v10.txt'],\n",
       "  [[[0, 24, 0.0], [1, 976, 1.0]], 'Alarm10_s1000_v4.txt'],\n",
       "  [[[0, 22, 0.0], [1, 978, 1.0]], 'Alarm10_s1000_v3.txt'],\n",
       "  [[[0, 34, 0.0], [1, 966, 1.0]], 'Alarm10_s1000_v7.txt'],\n",
       "  [[[0, 18, 0.0], [1, 982, 1.0]], 'Alarm10_s1000_v2.txt'],\n",
       "  [[[0, 31, 0.0], [1, 969, 1.0]], 'Alarm10_s1000_v8.txt']],\n",
       " 'filename': 'Alarm10_s1000_v9.txt'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PC_mean_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging scored results\n",
    "from operator import itemgetter\n",
    "log_directory=\"/home/a20114261/alarm_datasets/log_results/mmpc_partitioned\"\n",
    "ff = open(log_directory+'/scored_true-mb_log.txt','w')\n",
    "for e in sorted(PC_mean_score, key=itemgetter('filename'),reverse=True):\n",
    "    ff.write(e['filename']+';')\n",
    "    ff.write(e['TargetNode']+';')\n",
    "    for node in e['PC']:\n",
    "        ff.write(node+'_')\n",
    "    ff.write(';')\n",
    "    for class_pred in e['class_precisions']:\n",
    "            ff.write(class_pred[1]+'_')\n",
    "            for arr_acc in class_pred[0]:\n",
    "                ff.write(str(arr_acc[0])+'\\t')\n",
    "                ff.write(str(arr_acc[1])+'\\t')\n",
    "                ff.write(str(arr_acc[2])+'\\t')\n",
    "            ff.write('_')\n",
    "    ff.write('\\n')\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CandidateScore_Parallel(PobDict):\n",
    "\n",
    "    # training and scoring\n",
    "    X_train_df = pd.read_csv(main_directory+PobDict['filename'],delimiter='  ',header=None)\n",
    "    X_train_df.columns=graph_heads\n",
    "    \n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(X_train_df, X_train_df[TargetEvalNode], test_size=0.1, random_state=0)\n",
    "    \n",
    "    X_train = X_train_df\n",
    "    Y_train = X_train_df[PobDict['TargetNode']]\n",
    "    clf = LinearSVC()\n",
    "    \n",
    "    try:\n",
    "        clf.fit(X_train[PobDict['PC']],Y_train)\n",
    "    except:\n",
    "        PobDict['class_precisions']=[]\n",
    "        return PobDict\n",
    "        \n",
    "    \n",
    "    \n",
    "    PobDict['class_precisions']=[]\n",
    "    for filename in [x for x in os.listdir(main_directory) if x.split('_')[1]==PobDict['filename'].split('_')[1]]:\n",
    "        print(len([x for x in os.listdir(main_directory) if x.split('_')[1]==PobDict['filename'].split('_')[1]]))\n",
    "        if (filename == PobDict['filename']):\n",
    "            continue\n",
    "        print(\"*****\")\n",
    "        print()\n",
    "        print(\"Testing on dataset: \"+filename)\n",
    "        X_test = pd.read_csv(main_directory+filename,delimiter='  ',header=None)\n",
    "        X_test.columns = graph_heads\n",
    "        \n",
    "\n",
    "        # get score values\n",
    "        precision = []\n",
    "        values_counter=set(X_train_df[PobDict['TargetNode']])\n",
    "        for val in values_counter:\n",
    "            #Y_pred_c = clf.predict(X_train_df.query( TargetEvalNode+'== '+str(i))[PobDict['supercpc']][-100:])\n",
    "            if (len(X_test.query( PobDict['TargetNode']+'== '+str(val)))==0):\n",
    "                precision.append([val,0,0])\n",
    "                continue\n",
    "            Y_pred_c = clf.predict(X_test.query( PobDict['TargetNode']+'== '+str(val))[PobDict['PC']])\n",
    "            precision.append([val,len(Y_pred_c),Y_pred_c.tolist().count(val)/len(Y_pred_c)])\n",
    "\n",
    "        for e in range(0,len(precision)):\n",
    "            print('precision for class '+str(precision[e][0])+' with '+str(precision[e][1])+' samples in dataset : '+str(precision[e][2]))\n",
    "    \n",
    "        print(\"G-mean score: \"+str(g_mean(precision)))\n",
    "        print(\"*****\")\n",
    "        print()\n",
    "        #print(clf.score(X_test[PobDict['supercpc'][:10]],Y_test))\n",
    "        PobDict['class_precisions'].append([precision,filename])\n",
    "    return PobDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_scoring(size1, size2):\n",
    "    f_size1 = float(size1)\n",
    "    f_size2 = float(size2)\n",
    "    if (f_size1/f_size2 <= 1):\n",
    "        return f_size1/f_size2\n",
    "    else:\n",
    "        return f_size2/f_size1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_mean(precision_arr):\n",
    "    counter=1\n",
    "    for e in precision_arr:\n",
    "        counter=counter*e[2]\n",
    "    \n",
    "    return counter**(1/len(precision_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
